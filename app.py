import streamlit as st
import asyncio
import edge_tts
import os
from groq import Groq
from streamlit_mic_recorder import mic_recorder
import tempfile

# --- 1. é é¢èˆ‡å­—é«”è¨­å®š (CSS é­”æ³•) ---
st.set_page_config(page_title="Mama AI", page_icon="ğŸ‡ºğŸ‡¸", layout="centered")

# é€™è£¡å¼·åˆ¶æŠŠå­—é«”æ”¾å¤§åˆ° 24pxï¼Œè®“ä½ ä¸ç”¨æˆ´è€èŠ±çœ¼é¡
st.markdown("""
    <style>
    .stChatMessage p {
        font-size: 24px !important;
        line-height: 1.5 !important;
    }
    </style>
    """, unsafe_allow_html=True)

st.title("ğŸ‡ºğŸ‡¸ Mama AI")
st.caption("Press Start -> Speak English -> Press Stop")

# --- 2. ç²å– API Key ---
try:
    api_key = st.secrets["GROQ_API_KEY"]
except:
    st.error("è«‹åœ¨ Streamlit è¨­å®š Secrets: GROQ_API_KEY")
    st.stop()

client = Groq(api_key=api_key)

# --- 3. åˆå§‹åŒ– ---
if "messages" not in st.session_state:
    st.session_state.messages = [{
        "role": "system", 
        "content": "You are a patient American parent. I am your child. 1. NEVER speak Chinese. 2. Use simple words. 3. Correct me gently. 4. Keep answers short."
    }]

# --- 4. TTS å‡½æ•¸ ---
async def text_to_speech(text):
    communicate = edge_tts.Communicate(text, "en-US-AnaNeural", rate="-10%")
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as fp:
        await communicate.save(fp.name)
        return fp.name

# --- 5. ä»‹é¢é¡¯ç¤º ---
# é¡¯ç¤ºæ­·å²å°è©±
for msg in st.session_state.messages:
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

st.write("---")
st.write("### ğŸ‘‡ Click to Speak (æŒ‰ä¸€ä¸‹é–‹å§‹ï¼Œè¬›å®ŒæŒ‰åœæ­¢)")

# éŒ„éŸ³æŒ‰éˆ•
c1, c2 = st.columns([1, 3])
with c1:
    audio = mic_recorder(
        start_prompt="ğŸ”´ Start Recording",
        stop_prompt="â¹ï¸ Stop & Send",
        key='recorder'
    )

# --- 6. è™•ç†é‚è¼¯ ---
if audio:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as fp:
        fp.write(audio['bytes'])
        audio_filename = fp.name

    try:
        # è½ (Whisper Large V3)
        with open(audio_filename, "rb") as file:
            transcription = client.audio.transcriptions.create(
                file=(audio_filename, file.read()),
                model="whisper-large-v3",
                response_format="json"
            )
        user_text = transcription.text
        
        # é˜²å‘†æ©Ÿåˆ¶ï¼šå¦‚æœæ²’è½åˆ°è²éŸ³æˆ–è½åˆ°å¹»è¦ºï¼Œå°±ä¸å›æ‡‰
        if len(user_text) < 2 or "Halo" in user_text or "Amara" in user_text:
            st.warning("âš ï¸ I didn't hear you clearly. Please speak louder! (æ²’è½æ¸…æ¥šï¼Œè«‹å¤§è²ä¸€é»)")
        else:
            # é¡¯ç¤ºä½¿ç”¨è€…èªªçš„è©±
            st.session_state.messages.append({"role": "user", "content": user_text})
            with st.chat_message("user"):
                st.write(user_text)

            # æƒ³ (Llama 3.1)
            completion = client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=st.session_state.messages,
                temperature=0.7,
                max_tokens=200,
            )
            ai_response = completion.choices[0].message.content
            
            # é¡¯ç¤º AI å›æ‡‰
            st.session_state.messages.append({"role": "assistant", "content": ai_response})
            with st.chat_message("assistant"):
                st.write(ai_response)

            # èªª (Edge TTS)
            audio_file = asyncio.run(text_to_speech(ai_response))
            st.audio(audio_file, format="audio/mp3", autoplay=True)

    except Exception as e:
        st.error(f"Error: {str(e)}")
